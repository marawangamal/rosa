config:
   user: marawan.gamal # your username to check slurm status
   max_jobs: 10 # maximum number of jobs to run in parallel
   max_gpus: 10 # maximum number of GPUs to use in parallel

common_preamble_declarations:
  - "#!/bin/bash"
  - "#SBATCH --output=/home/mila/m/marawan.gamal/projects/rosa/outputs/slurm-%j.out"
  - "#SBATCH --error=/home/mila/m/marawan.gamal/projects/rosa/outputs/slurm-error-%j.out"
  - "#SBATCH --mem=10G                                         # Ask for 10 GB of RAM"

common_preamble_runs:
  # 1. Load the required modules
  - module load python/3.8

  # 2. Load your environment
  - source /home/mila/m/marawan.gamal/.venv/rosa/bin/activate

  # 3. Copy your dataset on the compute node
  - cp -r /home/mila/m/marawan.gamal/.cache/huggingface $SLURM_TMPDIR/huggingface

groups:

# =============================
# E2E_NLG BENCHMARK EXPERIMENTS
# =============================

#  - name: MET | E2E_NLG | GPT2-M (peft-method, rank, lr)
#    preamble:
#      - "#SBATCH --gres=gpu:2"
#      - "#SBATCH --time=8:00:00"
#    paralleljobs:
#      - python train_clm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan model.name=gpt2-medium fnmodel.name=none train.lr=5e-5
#      - python train_clm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan model.name=gpt2-medium fnmodel.name=ia3 train.lr=2e-4
#      - python train_clm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan model.name=gpt2-medium fnmodel.name=ia3 train.lr=2e-3
#      - python train_clm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan model.name=gpt2-medium fnmodel.name=ia3 train.lr=2e-2
#      - python train_clm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan model.name=gpt2-medium fnmodel.name=rosa fnmodel.params.rank=1 train.lr=2e-4
#      - python train_clm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan model.name=gpt2-medium fnmodel.name=rosa fnmodel.params.rank=1 train.lr=2e-3
#      - python train_clm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan model.name=gpt2-medium fnmodel.name=rosa fnmodel.params.rank=1 train.lr=2e-2
#      - python train_clm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan model.name=gpt2-medium fnmodel.name=rosa fnmodel.params.rank=1 train.lr=2e-1
#      - python train_clm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan model.name=gpt2-medium fnmodel.name=lora fnmodel.params.rank=1 train.lr=2e-4
#      - python train_clm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan model.name=gpt2-medium fnmodel.name=lora fnmodel.params.rank=1 train.lr=2e-3
#      - python train_clm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan model.name=gpt2-medium fnmodel.name=lora fnmodel.params.rank=1 train.lr=2e-2
#      - python train_clm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan model.name=gpt2-medium fnmodel.name=lora fnmodel.params.rank=1 train.lr=2e-1
#      - python train_clm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan model.name=gpt2-medium fnmodel.name=rosa fnmodel.params.rank=2 train.lr=2e-4
#      - python train_clm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan model.name=gpt2-medium fnmodel.name=rosa fnmodel.params.rank=2 train.lr=2e-3
#      - python train_clm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan model.name=gpt2-medium fnmodel.name=rosa fnmodel.params.rank=2 train.lr=2e-2
#      - python train_clm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan model.name=gpt2-medium fnmodel.name=rosa fnmodel.params.rank=2 train.lr=2e-1
#      - python train_clm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan model.name=gpt2-medium fnmodel.name=lora fnmodel.params.rank=2 train.lr=2e-4
#      - python train_clm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan model.name=gpt2-medium fnmodel.name=lora fnmodel.params.rank=2 train.lr=2e-3
#      - python train_clm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan model.name=gpt2-medium fnmodel.name=lora fnmodel.params.rank=2 train.lr=2e-2
#      - python train_clm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan model.name=gpt2-medium fnmodel.name=lora fnmodel.params.rank=2 train.lr=2e-1
#      - python train_clm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan model.name=gpt2-medium fnmodel.name=rosa fnmodel.params.rank=4 train.lr=2e-4
#      - python train_clm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan model.name=gpt2-medium fnmodel.name=rosa fnmodel.params.rank=4 train.lr=2e-3
#      - python train_clm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan model.name=gpt2-medium fnmodel.name=rosa fnmodel.params.rank=4 train.lr=2e-2
#      - python train_clm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan model.name=gpt2-medium fnmodel.name=rosa fnmodel.params.rank=4 train.lr=2e-1
#      - python train_clm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan model.name=gpt2-medium fnmodel.name=lora fnmodel.params.rank=4 train.lr=2e-4
#      - python train_clm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan model.name=gpt2-medium fnmodel.name=lora fnmodel.params.rank=4 train.lr=2e-3
#      - python train_clm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan model.name=gpt2-medium fnmodel.name=lora fnmodel.params.rank=4 train.lr=2e-2
#      - python train_clm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan model.name=gpt2-medium fnmodel.name=lora fnmodel.params.rank=4 train.lr=2e-1
#      - python train_clm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan model.name=gpt2-medium fnmodel.name=rosa fnmodel.params.rank=8 train.lr=2e-4
#      - python train_clm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan model.name=gpt2-medium fnmodel.name=rosa fnmodel.params.rank=8 train.lr=2e-3
#      - python train_clm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan model.name=gpt2-medium fnmodel.name=rosa fnmodel.params.rank=8 train.lr=2e-2
#      - python train_clm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan model.name=gpt2-medium fnmodel.name=rosa fnmodel.params.rank=8 train.lr=2e-1
#      - python train_clm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan model.name=gpt2-medium fnmodel.name=lora fnmodel.params.rank=8 train.lr=2e-4
#      - python train_clm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan model.name=gpt2-medium fnmodel.name=lora fnmodel.params.rank=8 train.lr=2e-3
#      - python train_clm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan model.name=gpt2-medium fnmodel.name=lora fnmodel.params.rank=8 train.lr=2e-2
#      - python train_clm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan model.name=gpt2-medium fnmodel.name=lora fnmodel.params.rank=8 train.lr=2e-1

#  - name: MET | E2E_NLG | GPT2-M w/ ROSA (sample method, lr)
#    preamble:
#      - "#SBATCH --gres=gpu:2"
#      - "#SBATCH --time=8:00:00"
#    paralleljobs:
#      - python train_clm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan model.name=gpt2-medium fnmodel.name=rosa fnmodel.params.rank=1 train.lr=2e-4
#      - python train_clm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan model.name=gpt2-medium fnmodel.name=rosa fnmodel.params.rank=1 train.lr=2e-3
#      - python train_clm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan model.name=gpt2-medium fnmodel.name=rosa fnmodel.params.rank=1 train.lr=2e-2
#      - python train_clm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan model.name=gpt2-medium fnmodel.name=rosa fnmodel.params.rank=1 fnmodel.params.factorize_mode=top train.lr=2e-4
#      - python train_clm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan model.name=gpt2-medium fnmodel.name=rosa fnmodel.params.rank=1 fnmodel.params.factorize_mode=top train.lr=2e-3
#      - python train_clm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan model.name=gpt2-medium fnmodel.name=rosa fnmodel.params.rank=1 fnmodel.params.factorize_mode=top train.lr=2e-2
#      - python train_clm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan model.name=gpt2-medium fnmodel.name=rosa fnmodel.params.rank=1 fnmodel.params.factorize_mode=bottom train.lr=2e-4
#      - python train_clm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan model.name=gpt2-medium fnmodel.name=rosa fnmodel.params.rank=1 fnmodel.params.factorize_mode=bottom train.lr=2e-3
#      - python train_clm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan model.name=gpt2-medium fnmodel.name=rosa fnmodel.params.rank=1 fnmodel.params.factorize_mode=bottom train.lr=2e-2
#      - python train_clm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan model.name=gpt2-medium fnmodel.name=rosa fnmodel.params.rank=4 train.lr=2e-4
#      - python train_clm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan model.name=gpt2-medium fnmodel.name=rosa fnmodel.params.rank=4 train.lr=2e-3
#      - python train_clm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan model.name=gpt2-medium fnmodel.name=rosa fnmodel.params.rank=4 train.lr=2e-2
#      - python train_clm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan model.name=gpt2-medium fnmodel.name=rosa fnmodel.params.rank=4 fnmodel.params.factorize_mode=top train.lr=2e-4
#      - python train_clm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan model.name=gpt2-medium fnmodel.name=rosa fnmodel.params.rank=4 fnmodel.params.factorize_mode=top train.lr=2e-3
#      - python train_clm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan model.name=gpt2-medium fnmodel.name=rosa fnmodel.params.rank=4 fnmodel.params.factorize_mode=top train.lr=2e-2
#      - python train_clm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan model.name=gpt2-medium fnmodel.name=rosa fnmodel.params.rank=4 fnmodel.params.factorize_mode=bottom train.lr=2e-4
#      - python train_clm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan model.name=gpt2-medium fnmodel.name=rosa fnmodel.params.rank=4 fnmodel.params.factorize_mode=bottom train.lr=2e-3
#      - python train_clm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan model.name=gpt2-medium fnmodel.name=rosa fnmodel.params.rank=4 fnmodel.params.factorize_mode=bottom train.lr=2e-2


#  - name: GLUE | COLA | Roberta-Base (peft-method, lr, adapt_method)
#    preamble:
#      - "#SBATCH --gres=gpu:2"
#      - "#SBATCH --time=8:00:00"
#    paralleljobs:
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan runs=3 seed=42 fnmodel.name=lora +task=cola fnmodel.params.rank=2 fnmodel.params.adapt_method=a train.lr=2e-4
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan runs=3 seed=42 fnmodel.name=rosa +task=cola fnmodel.params.rank=2 fnmodel.params.adapt_method=a train.lr=2e-3


        #  - name: GLUE | COLA | Roberta-Base (peft-method, rank, lr)
#    preamble:
#      - "#SBATCH --gres=gpu:2"
#      - "#SBATCH --time=8:00:00"
#    paralleljobs:
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 fnmodel.name=none +task=cola train.lr=2e-5
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 fnmodel.name=ia3 +task=cola train.lr=2e-3
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 fnmodel.name=lora +task=cola fnmodel.params.rank=2 train.epochs=10 train.lr=2e-4
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 fnmodel.name=rosa +task=cola fnmodel.params.rank=2 train.epochs=10 train.lr=2e-3
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 fnmodel.name=lora +task=cola fnmodel.params.rank=4 train.epochs=10 train.lr=2e-4
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 fnmodel.name=rosa +task=cola fnmodel.params.rank=4 train.epochs=10 train.lr=2e-3
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 fnmodel.name=lora +task=cola fnmodel.params.rank=8 train.epochs=10 train.lr=2e-4
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 fnmodel.name=rosa +task=cola fnmodel.params.rank=8 train.epochs=10 train.lr=2e-3

# ==========================
# GLUE BENCHMARK EXPERIMENTS
# ==========================

  - name: GLUE | COLA | Roberta-Base (peft_method, rank, factorization_freq, factorize_mode)
    preamble:
      - "#SBATCH --gres=gpu:1"
      - "#SBATCH --partition=unkillable"
    paralleljobs:
# baseline
      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 train.lr=2e-5
# rank2
      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface seed=42 +profile=marawan +task=cola pnmodel.name=lora pnmodel.params.rank=2 train.lr=2e-2
      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface seed=42 +profile=marawan +task=cola pnmodel.name=lora pnmodel.params.rank=2 train.lr=2e-3
      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface seed=42 +profile=marawan +task=cola pnmodel.name=lora pnmodel.params.rank=2 train.lr=2e-4
      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface seed=42 +profile=marawan +task=cola pnmodel.name=lora pnmodel.params.rank=2 pnmodel.params.factorize_method=random train.lr=2e-2
      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface seed=42 +profile=marawan +task=cola pnmodel.name=lora pnmodel.params.rank=2 pnmodel.params.factorize_method=random train.lr=2e-3
      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface seed=42 +profile=marawan +task=cola pnmodel.name=lora pnmodel.params.rank=2 pnmodel.params.factorize_method=random train.lr=2e-4
      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface seed=42 +profile=marawan +task=cola pnmodel.name=rosa pnmodel.params.rank=2 train.lr=2e-2
      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface seed=42 +profile=marawan +task=cola pnmodel.name=rosa pnmodel.params.rank=2 train.lr=2e-3
      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface seed=42 +profile=marawan +task=cola pnmodel.name=rosa pnmodel.params.rank=2 train.lr=2e-4


#  - name: GLUE | COLA | Roberta-Base (peft_method, rank, factorization_freq, factorize_mode)
#    preamble:
#      - "#SBATCH --gres=gpu:1"
#      - "#SBATCH --partition=unkillable"
#    paralleljobs:
# baseline
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan runs=3 seed=42 train.lr=2e-6
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan runs=3 seed=42 train.lr=2e-5
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan runs=3 seed=42 train.lr=2e-4
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan runs=3 seed=42 fnmodel.name=ia3 +task=cola fnmodel.params.rank=2 train.lr=2e-2
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan runs=3 seed=42 fnmodel.name=ia3 +task=cola fnmodel.params.rank=2 train.lr=2e-3
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan runs=3 seed=42 fnmodel.name=ia3 +task=cola fnmodel.params.rank=2 train.lr=2e-4
# rank2
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan runs=3 seed=42 fnmodel.name=lora +task=cola fnmodel.params.rank=2 train.lr=2e-2
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan runs=3 seed=42 fnmodel.name=lora +task=cola fnmodel.params.rank=2 train.lr=2e-3
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan runs=3 seed=42 fnmodel.name=lora +task=cola fnmodel.params.rank=2 train.lr=2e-4
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan runs=3 seed=42 fnmodel.name=rosa +task=cola fnmodel.params.rank=2 train.lr=2e-2
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan runs=3 seed=42 fnmodel.name=rosa +task=cola fnmodel.params.rank=2 train.lr=2e-3
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan runs=3 seed=42 fnmodel.name=rosa +task=cola fnmodel.params.rank=2 train.lr=2e-4
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan runs=3 seed=42 fnmodel.name=rosa +task=cola fnmodel.params.rank=2 fnmodel.params.factorize_mode=top train.lr=2e-2
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan runs=3 seed=42 fnmodel.name=rosa +task=cola fnmodel.params.rank=2 fnmodel.params.factorize_mode=top train.lr=2e-3
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan runs=3 seed=42 fnmodel.name=rosa +task=cola fnmodel.params.rank=2 fnmodel.params.factorize_mode=top train.lr=2e-4
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan runs=3 seed=42 fnmodel.name=rosa +task=cola fnmodel.params.rank=2 fnmodel.params.factorize_mode=bottom train.lr=2e-2
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan runs=3 seed=42 fnmodel.name=rosa +task=cola fnmodel.params.rank=2 fnmodel.params.factorize_mode=bottom train.lr=2e-3
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan runs=3 seed=42 fnmodel.name=rosa +task=cola fnmodel.params.rank=2 fnmodel.params.factorize_mode=bottom train.lr=2e-4
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan runs=3 seed=42 fnmodel.name=rosa +task=cola fnmodel.params.rank=2 fnmodel.factorize_freq=2 train.lr=2e-2
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan runs=3 seed=42 fnmodel.name=rosa +task=cola fnmodel.params.rank=2 fnmodel.factorize_freq=2 train.lr=2e-3
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan runs=3 seed=42 fnmodel.name=rosa +task=cola fnmodel.params.rank=2 fnmodel.factorize_freq=2 train.lr=2e-4
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan runs=3 seed=42 fnmodel.name=rosa +task=cola fnmodel.params.rank=2 fnmodel.factorize_freq=20 train.lr=2e-2
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan runs=3 seed=42 fnmodel.name=rosa +task=cola fnmodel.params.rank=2 fnmodel.factorize_freq=20 train.lr=2e-3
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan runs=3 seed=42 fnmodel.name=rosa +task=cola fnmodel.params.rank=2 fnmodel.factorize_freq=20 train.lr=2e-4
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan runs=3 seed=42 fnmodel.name=rosa +task=cola fnmodel.params.rank=2 fnmodel.factorize_freq=20 fnmodel.params.factorize_method=add train.lr=2e-2
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan runs=3 seed=42 fnmodel.name=rosa +task=cola fnmodel.params.rank=2 fnmodel.factorize_freq=20 fnmodel.params.factorize_method=add train.lr=2e-3
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan runs=3 seed=42 fnmodel.name=rosa +task=cola fnmodel.params.rank=2 fnmodel.factorize_freq=20 fnmodel.params.factorize_method=add train.lr=2e-4
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan runs=3 seed=42 fnmodel.name=rosa +task=cola fnmodel.params.rank=2 fnmodel.factorize_freq=20 fnmodel.params.factorize_method=add train.lr=2e-5
# rank8
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan runs=3 seed=42 fnmodel.name=lora +task=cola fnmodel.params.rank=8 train.lr=2e-2
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan runs=3 seed=42 fnmodel.name=lora +task=cola fnmodel.params.rank=8 train.lr=2e-3
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan runs=3 seed=42 fnmodel.name=lora +task=cola fnmodel.params.rank=8 train.lr=2e-4
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan runs=3 seed=42 fnmodel.name=rosa +task=cola fnmodel.params.rank=8 train.lr=2e-2
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan runs=3 seed=42 fnmodel.name=rosa +task=cola fnmodel.params.rank=8 train.lr=2e-3
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan runs=3 seed=42 fnmodel.name=rosa +task=cola fnmodel.params.rank=8 train.lr=2e-4
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan runs=3 seed=42 fnmodel.name=rosa +task=cola fnmodel.params.rank=8 fnmodel.params.factorize_mode=top train.lr=2e-2
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan runs=3 seed=42 fnmodel.name=rosa +task=cola fnmodel.params.rank=8 fnmodel.params.factorize_mode=top train.lr=2e-3
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan runs=3 seed=42 fnmodel.name=rosa +task=cola fnmodel.params.rank=8 fnmodel.params.factorize_mode=top train.lr=2e-4
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan runs=3 seed=42 fnmodel.name=rosa +task=cola fnmodel.params.rank=8 fnmodel.params.factorize_mode=bottom train.lr=2e-2
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan runs=3 seed=42 fnmodel.name=rosa +task=cola fnmodel.params.rank=8 fnmodel.params.factorize_mode=bottom train.lr=2e-3
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan runs=3 seed=42 fnmodel.name=rosa +task=cola fnmodel.params.rank=8 fnmodel.params.factorize_mode=bottom train.lr=2e-4
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan runs=3 seed=42 fnmodel.name=rosa +task=cola fnmodel.params.rank=8 fnmodel.factorize_freq=2 train.lr=2e-2
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan runs=3 seed=42 fnmodel.name=rosa +task=cola fnmodel.params.rank=8 fnmodel.factorize_freq=2 train.lr=2e-3
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan runs=3 seed=42 fnmodel.name=rosa +task=cola fnmodel.params.rank=8 fnmodel.factorize_freq=2 train.lr=2e-4
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan runs=3 seed=42 fnmodel.name=rosa +task=cola fnmodel.params.rank=8 fnmodel.factorize_freq=20 train.lr=2e-2
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan runs=3 seed=42 fnmodel.name=rosa +task=cola fnmodel.params.rank=8 fnmodel.factorize_freq=20 train.lr=2e-3
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan runs=3 seed=42 fnmodel.name=rosa +task=cola fnmodel.params.rank=8 fnmodel.factorize_freq=20 train.lr=2e-4
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan runs=3 seed=42 fnmodel.name=rosa +task=cola fnmodel.params.rank=8 fnmodel.factorize_freq=20 fnmodel.params.factorize_method=add train.lr=2e-2
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan runs=3 seed=42 fnmodel.name=rosa +task=cola fnmodel.params.rank=8 fnmodel.factorize_freq=20 fnmodel.params.factorize_method=add train.lr=2e-3
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan runs=3 seed=42 fnmodel.name=rosa +task=cola fnmodel.params.rank=8 fnmodel.factorize_freq=20 fnmodel.params.factorize_method=add train.lr=2e-4
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan runs=3 seed=42 fnmodel.name=rosa +task=cola fnmodel.params.rank=8 fnmodel.factorize_freq=20 fnmodel.params.factorize_method=add train.lr=2e-5

#  - name: GLUE | COLA | Roberta-Base (peft_method, rank, factorization_freq, factorize_mode)
#    preamble:
#      - "#SBATCH --gres=gpu:1"
#      - "#SBATCH --partition=unkillable"
#    paralleljobs:
## baseline
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan runs=3 seed=42 +task=cola fnmodel.name=rosa fnmodel.params.bias_requires_grad=False fnmodel.params.rank=8 train.lr=2e-3 fnmodel.factorize_freq=1
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan runs=3 seed=42 +task=cola fnmodel.name=rosa fnmodel.params.bias_requires_grad=False fnmodel.params.rank=8 train.lr=2e-3 fnmodel.factorize_freq=2
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan runs=3 seed=42 +task=cola fnmodel.name=rosa fnmodel.params.bias_requires_grad=False fnmodel.params.rank=8 train.lr=2e-3 fnmodel.factorize_freq=3
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan runs=3 seed=42 +task=cola fnmodel.name=rosa fnmodel.params.bias_requires_grad=False fnmodel.params.rank=8 train.lr=2e-3 fnmodel.factorize_freq=4
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan runs=3 seed=42 +task=cola fnmodel.name=rosa fnmodel.params.bias_requires_grad=False fnmodel.params.rank=2 train.lr=2e-3 fnmodel.factorize_freq=1
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan runs=3 seed=42 +task=cola fnmodel.name=rosa fnmodel.params.bias_requires_grad=False fnmodel.params.rank=2 train.lr=2e-3 fnmodel.factorize_freq=2
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan runs=3 seed=42 +task=cola fnmodel.name=rosa fnmodel.params.bias_requires_grad=False fnmodel.params.rank=2 train.lr=2e-3 fnmodel.factorize_freq=3
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan runs=3 seed=42 +task=cola fnmodel.name=rosa fnmodel.params.bias_requires_grad=False fnmodel.params.rank=2 train.lr=2e-3 fnmodel.factorize_freq=4

#
#  - name: GLUE | QNLI | Roberta-Base (peft-method, rank, lr)
#    preamble:
#      - "#SBATCH --gres=gpu:1"
#      - "#SBATCH --partition=unkillable"
#    paralleljobs:
# baseline
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=qnli train.epochs=10  train.batch_size=16 fnmodel.name=none train.lr=2e-5
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=qnli train.epochs=10  train.batch_size=16 fnmodel.name=none train.lr=2e-4
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=qnli train.epochs=10  train.batch_size=16 fnmodel.name=ia3 fnmodel.params.rank=2 train.lr=2e-5
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=qnli train.epochs=10  train.batch_size=16 fnmodel.name=ia3 fnmodel.params.rank=2 train.lr=2e-4
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=qnli train.epochs=10  train.batch_size=16 fnmodel.name=ia3 fnmodel.params.rank=2 train.lr=2e-3
# rank2
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=qnli train.epochs=10  train.batch_size=16 fnmodel.name=lora fnmodel.params.rank=2 train.lr=2e-5
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=qnli train.epochs=10  train.batch_size=16 fnmodel.name=lora fnmodel.params.rank=2 train.lr=2e-4
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=qnli train.epochs=10  train.batch_size=16 fnmodel.name=lora fnmodel.params.rank=2 train.lr=2e-3
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=qnli train.epochs=10  train.batch_size=16 fnmodel.name=rosa fnmodel.params.rank=2 train.lr=2e-5
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=qnli train.epochs=10  train.batch_size=16 fnmodel.name=rosa fnmodel.params.rank=2 train.lr=2e-4
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=qnli train.epochs=10  train.batch_size=16 fnmodel.name=rosa fnmodel.params.rank=2 train.lr=2e-3
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=qnli train.epochs=10  train.batch_size=16 fnmodel.name=rosa fnmodel.params.rank=2 train.lr=2e-5 fnmodel.factorize_freq=2
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=qnli train.epochs=10  train.batch_size=16 fnmodel.name=rosa fnmodel.params.rank=2 train.lr=2e-4 fnmodel.factorize_freq=2
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=qnli train.epochs=10  train.batch_size=16 fnmodel.name=rosa fnmodel.params.rank=2 train.lr=2e-3 fnmodel.factorize_freq=2
# rank8
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=qnli train.epochs=10  train.batch_size=16 fnmodel.name=lora fnmodel.params.rank=8 train.lr=2e-5
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=qnli train.epochs=10  train.batch_size=16 fnmodel.name=lora fnmodel.params.rank=8 train.lr=2e-4
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=qnli train.epochs=10  train.batch_size=16 fnmodel.name=lora fnmodel.params.rank=8 train.lr=2e-3
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=qnli train.epochs=10  train.batch_size=16 fnmodel.name=rosa fnmodel.params.rank=8 train.lr=2e-5
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=qnli train.epochs=10  train.batch_size=16 fnmodel.name=rosa fnmodel.params.rank=8 train.lr=2e-4
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=qnli train.epochs=10  train.batch_size=16 fnmodel.name=rosa fnmodel.params.rank=8 train.lr=2e-3
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=qnli train.epochs=10  train.batch_size=16 fnmodel.name=rosa fnmodel.params.rank=8 train.lr=2e-5 fnmodel.factorize_freq=2
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=qnli train.epochs=10  train.batch_size=16 fnmodel.name=rosa fnmodel.params.rank=8 train.lr=2e-4 fnmodel.factorize_freq=2
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=qnli train.epochs=10  train.batch_size=16 fnmodel.name=rosa fnmodel.params.rank=8 train.lr=2e-3 fnmodel.factorize_freq=2

#  - name: GLUE | MNLI | Roberta-Base (peft-method, rank, lr)
#    preamble:
#      - "#SBATCH --gres=gpu:1"
#      - "#SBATCH --partition=unkillable"
#    paralleljobs:
# baseline
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=mnli train.epochs=10  train.batch_size=16 fnmodel.name=none train.lr=2e-5
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=mnli train.epochs=10  train.batch_size=16 fnmodel.name=none train.lr=2e-4
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=mnli train.epochs=10  train.batch_size=16 fnmodel.name=ia3 fnmodel.params.rank=2 train.lr=2e-5
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=mnli train.epochs=10  train.batch_size=16 fnmodel.name=ia3 fnmodel.params.rank=2 train.lr=2e-4
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=mnli train.epochs=10  train.batch_size=16 fnmodel.name=ia3 fnmodel.params.rank=2 train.lr=2e-3
# rank2
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=mnli train.epochs=10  train.batch_size=16 fnmodel.name=lora fnmodel.params.rank=2 train.lr=2e-5
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=mnli train.epochs=10  train.batch_size=16 fnmodel.name=lora fnmodel.params.rank=2 train.lr=2e-4
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=mnli train.epochs=10  train.batch_size=16 fnmodel.name=lora fnmodel.params.rank=2 train.lr=2e-3
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=mnli train.epochs=10  train.batch_size=16 fnmodel.name=rosa fnmodel.params.rank=2 train.lr=2e-5
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=mnli train.epochs=10  train.batch_size=16 fnmodel.name=rosa fnmodel.params.rank=2 train.lr=2e-4
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=mnli train.epochs=10  train.batch_size=16 fnmodel.name=rosa fnmodel.params.rank=2 train.lr=2e-3
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=mnli train.epochs=10  train.batch_size=16 fnmodel.name=rosa fnmodel.params.rank=2 train.lr=2e-5 fnmodel.factorize_freq=2
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=mnli train.epochs=10  train.batch_size=16 fnmodel.name=rosa fnmodel.params.rank=2 train.lr=2e-4 fnmodel.factorize_freq=2
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=mnli train.epochs=10  train.batch_size=16 fnmodel.name=rosa fnmodel.params.rank=2 train.lr=2e-3 fnmodel.factorize_freq=2
# rank8
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=mnli train.epochs=10  train.batch_size=16 fnmodel.name=lora fnmodel.params.rank=8 train.lr=2e-5
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=mnli train.epochs=10  train.batch_size=16 fnmodel.name=lora fnmodel.params.rank=8 train.lr=2e-4
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=mnli train.epochs=10  train.batch_size=16 fnmodel.name=lora fnmodel.params.rank=8 train.lr=2e-3
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=mnli train.epochs=10  train.batch_size=16 fnmodel.name=rosa fnmodel.params.rank=8 train.lr=2e-5
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=mnli train.epochs=10  train.batch_size=16 fnmodel.name=rosa fnmodel.params.rank=8 train.lr=2e-4
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=mnli train.epochs=10  train.batch_size=16 fnmodel.name=rosa fnmodel.params.rank=8 train.lr=2e-3
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=mnli train.epochs=10  train.batch_size=16 fnmodel.name=rosa fnmodel.params.rank=8 train.lr=2e-5 fnmodel.factorize_freq=2
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=mnli train.epochs=10  train.batch_size=16 fnmodel.name=rosa fnmodel.params.rank=8 train.lr=2e-4 fnmodel.factorize_freq=2
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=mnli train.epochs=10  train.batch_size=16 fnmodel.name=rosa fnmodel.params.rank=8 train.lr=2e-3 fnmodel.factorize_freq=2


#  - name: GLUE | MRPC | Roberta-Base (peft-method, rank, lr)
#    preamble:
#      - "#SBATCH --gres=gpu:1"
#      - "#SBATCH --partition=unkillable"
#    paralleljobs:
# baseline
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=mrpc train.epochs=10  train.batch_size=16 fnmodel.name=none train.lr=2e-5
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=mrpc train.epochs=10  train.batch_size=16 fnmodel.name=none train.lr=2e-4
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=mrpc train.epochs=10  train.batch_size=16 fnmodel.name=ia3 fnmodel.params.rank=2 train.lr=2e-5
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=mrpc train.epochs=10  train.batch_size=16 fnmodel.name=ia3 fnmodel.params.rank=2 train.lr=2e-4
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=mrpc train.epochs=10  train.batch_size=16 fnmodel.name=ia3 fnmodel.params.rank=2 train.lr=2e-3
# rank2
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=mrpc train.epochs=10  train.batch_size=16 fnmodel.name=lora fnmodel.params.rank=2 train.lr=2e-5
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=mrpc train.epochs=10  train.batch_size=16 fnmodel.name=lora fnmodel.params.rank=2 train.lr=2e-4
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=mrpc train.epochs=10  train.batch_size=16 fnmodel.name=lora fnmodel.params.rank=2 train.lr=2e-3
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=mrpc train.epochs=10  train.batch_size=16 fnmodel.name=rosa fnmodel.params.rank=2 train.lr=2e-5
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=mrpc train.epochs=10  train.batch_size=16 fnmodel.name=rosa fnmodel.params.rank=2 train.lr=2e-4
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=mrpc train.epochs=10  train.batch_size=16 fnmodel.name=rosa fnmodel.params.rank=2 train.lr=2e-3
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=mrpc train.epochs=10  train.batch_size=16 fnmodel.name=rosa fnmodel.params.rank=2 train.lr=2e-5 fnmodel.factorize_freq=2
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=mrpc train.epochs=10  train.batch_size=16 fnmodel.name=rosa fnmodel.params.rank=2 train.lr=2e-4 fnmodel.factorize_freq=2
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=mrpc train.epochs=10  train.batch_size=16 fnmodel.name=rosa fnmodel.params.rank=2 train.lr=2e-3 fnmodel.factorize_freq=2
# rank8
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=mrpc train.epochs=10  train.batch_size=16 fnmodel.name=lora fnmodel.params.rank=8 train.lr=2e-5
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=mrpc train.epochs=10  train.batch_size=16 fnmodel.name=lora fnmodel.params.rank=8 train.lr=2e-4
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=mrpc train.epochs=10  train.batch_size=16 fnmodel.name=lora fnmodel.params.rank=8 train.lr=2e-3
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=mrpc train.epochs=10  train.batch_size=16 fnmodel.name=rosa fnmodel.params.rank=8 train.lr=2e-5
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=mrpc train.epochs=10  train.batch_size=16 fnmodel.name=rosa fnmodel.params.rank=8 train.lr=2e-4
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=mrpc train.epochs=10  train.batch_size=16 fnmodel.name=rosa fnmodel.params.rank=8 train.lr=2e-3
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=mrpc train.epochs=10  train.batch_size=16 fnmodel.name=rosa fnmodel.params.rank=8 train.lr=2e-5 fnmodel.factorize_freq=2
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=mrpc train.epochs=10  train.batch_size=16 fnmodel.name=rosa fnmodel.params.rank=8 train.lr=2e-4 fnmodel.factorize_freq=2
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=mrpc train.epochs=10  train.batch_size=16 fnmodel.name=rosa fnmodel.params.rank=8 train.lr=2e-3 fnmodel.factorize_freq=2

#  - name: GLUE | BOOLQ | Roberta-Base (peft-method, rank, lr, factorization_freq)
#    preamble:
#      - "#SBATCH --gres=gpu:1"
#      - "#SBATCH --partition=unkillable"
#    paralleljobs:
# baseline
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=boolq train.epochs=10 train.batch_size=32 fnmodel.name=none train.lr=2e-5
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=boolq train.epochs=10 train.batch_size=32 fnmodel.name=none train.lr=2e-4
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=boolq train.epochs=10 train.batch_size=32 fnmodel.name=none train.lr=2e-3
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=boolq train.epochs=10 train.batch_size=32 fnmodel.name=ia3 fnmodel.params.rank=2 train.lr=2e-5
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=boolq train.epochs=10 train.batch_size=32 fnmodel.name=ia3 fnmodel.params.rank=2 train.lr=2e-4
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=boolq train.epochs=10 train.batch_size=32 fnmodel.name=ia3 fnmodel.params.rank=2 train.lr=2e-3
# rank2
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=boolq train.epochs=10  train.batch_size=16 fnmodel.name=lora fnmodel.params.rank=2 train.lr=2e-5
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=boolq train.epochs=10  train.batch_size=16 fnmodel.name=lora fnmodel.params.rank=2 train.lr=2e-4
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=boolq train.epochs=10  train.batch_size=16 fnmodel.name=lora fnmodel.params.rank=2 train.lr=2e-3
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=boolq train.epochs=10  train.batch_size=16 fnmodel.name=rosa fnmodel.params.rank=2 train.lr=2e-5
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=boolq train.epochs=10  train.batch_size=16 fnmodel.name=rosa fnmodel.params.rank=2 train.lr=2e-4
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=boolq train.epochs=10  train.batch_size=16 fnmodel.name=rosa fnmodel.params.rank=2 train.lr=2e-3
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=boolq train.epochs=10  train.batch_size=16 fnmodel.name=rosa fnmodel.params.rank=2 train.lr=2e-5 fnmodel.factorize_freq=2
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=boolq train.epochs=10  train.batch_size=16 fnmodel.name=rosa fnmodel.params.rank=2 train.lr=2e-4 fnmodel.factorize_freq=2
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=boolq train.epochs=10  train.batch_size=16 fnmodel.name=rosa fnmodel.params.rank=2 train.lr=2e-3 fnmodel.factorize_freq=2
# rank8
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=boolq train.epochs=10  train.batch_size=16 fnmodel.name=lora fnmodel.params.rank=8 train.lr=2e-5
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=boolq train.epochs=10  train.batch_size=16 fnmodel.name=lora fnmodel.params.rank=8 train.lr=2e-4
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=boolq train.epochs=10  train.batch_size=16 fnmodel.name=lora fnmodel.params.rank=8 train.lr=2e-3
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=boolq train.epochs=10  train.batch_size=16 fnmodel.name=rosa fnmodel.params.rank=8 train.lr=2e-5
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=boolq train.epochs=10  train.batch_size=16 fnmodel.name=rosa fnmodel.params.rank=8 train.lr=2e-4
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=boolq train.epochs=10  train.batch_size=16 fnmodel.name=rosa fnmodel.params.rank=8 train.lr=2e-3
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=boolq train.epochs=10  train.batch_size=16 fnmodel.name=rosa fnmodel.params.rank=8 train.lr=2e-5 fnmodel.factorize_freq=2
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=boolq train.epochs=10  train.batch_size=16 fnmodel.name=rosa fnmodel.params.rank=8 train.lr=2e-4 fnmodel.factorize_freq=2
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=boolq train.epochs=10  train.batch_size=16 fnmodel.name=rosa fnmodel.params.rank=8 train.lr=2e-3 fnmodel.factorize_freq=2

#  - name: GLUE | RTE | Roberta-Base (peft-method, rank, lr, factorization_freq)
#    preamble:
#      - "#SBATCH --gres=gpu:1"
#      - "#SBATCH --partition=unkillable"
#    paralleljobs:
## baseline
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=rte train.epochs=10 train.batch_size=32 fnmodel.name=none train.lr=2e-5
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=rte train.epochs=10 train.batch_size=32 fnmodel.name=none train.lr=2e-4
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=rte train.epochs=10 train.batch_size=32 fnmodel.name=none train.lr=2e-3
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=rte train.epochs=10 train.batch_size=32 fnmodel.name=ia3 fnmodel.params.rank=2 train.lr=2e-5
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=rte train.epochs=10 train.batch_size=32 fnmodel.name=ia3 fnmodel.params.rank=2 train.lr=2e-4
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=rte train.epochs=10 train.batch_size=32 fnmodel.name=ia3 fnmodel.params.rank=2 train.lr=2e-3
## rank2
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=rte train.epochs=10  train.batch_size=16 fnmodel.name=lora fnmodel.params.rank=2 train.lr=2e-5
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=rte train.epochs=10  train.batch_size=16 fnmodel.name=lora fnmodel.params.rank=2 train.lr=2e-4
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=rte train.epochs=10  train.batch_size=16 fnmodel.name=lora fnmodel.params.rank=2 train.lr=2e-3
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=rte train.epochs=10  train.batch_size=16 fnmodel.name=rosa fnmodel.params.rank=2 train.lr=2e-5
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=rte train.epochs=10  train.batch_size=16 fnmodel.name=rosa fnmodel.params.rank=2 train.lr=2e-4
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=rte train.epochs=10  train.batch_size=16 fnmodel.name=rosa fnmodel.params.rank=2 train.lr=2e-3
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=rte train.epochs=10  train.batch_size=16 fnmodel.name=rosa fnmodel.params.rank=2 train.lr=2e-5 fnmodel.factorize_freq=2
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=rte train.epochs=10  train.batch_size=16 fnmodel.name=rosa fnmodel.params.rank=2 train.lr=2e-4 fnmodel.factorize_freq=2
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=rte train.epochs=10  train.batch_size=16 fnmodel.name=rosa fnmodel.params.rank=2 train.lr=2e-3 fnmodel.factorize_freq=2
# rank8
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=rte train.epochs=10  train.batch_size=16 fnmodel.name=lora fnmodel.params.bias_requires_grad=False fnmodel.params.rank=8 train.lr=2e-5
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=rte train.epochs=10  train.batch_size=16 fnmodel.name=lora fnmodel.params.bias_requires_grad=False fnmodel.params.rank=8 train.lr=2e-4
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=rte train.epochs=10  train.batch_size=16 fnmodel.name=lora fnmodel.params.bias_requires_grad=False fnmodel.params.rank=8 train.lr=2e-3
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=rte train.epochs=10  train.batch_size=16 fnmodel.name=rosa fnmodel.params.bias_requires_grad=False fnmodel.params.rank=8 train.lr=2e-5
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=rte train.epochs=10  train.batch_size=16 fnmodel.name=rosa fnmodel.params.bias_requires_grad=False fnmodel.params.rank=8 train.lr=2e-4
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=rte train.epochs=10  train.batch_size=16 fnmodel.name=rosa fnmodel.params.bias_requires_grad=False fnmodel.params.rank=8 train.lr=2e-3
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=rte train.epochs=10  train.batch_size=16 fnmodel.name=rosa fnmodel.params.bias_requires_grad=False fnmodel.params.rank=8 train.lr=2e-5 fnmodel.factorize_freq=2
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=rte train.epochs=10  train.batch_size=16 fnmodel.name=rosa fnmodel.params.bias_requires_grad=False fnmodel.params.rank=8 train.lr=2e-4 fnmodel.factorize_freq=2
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=rte train.epochs=10  train.batch_size=16 fnmodel.name=rosa fnmodel.params.bias_requires_grad=False fnmodel.params.rank=8 train.lr=2e-3 fnmodel.factorize_freq=2

#  - name: GLUE | STSB | Roberta-Base (peft-method, rank, lr, factorization_freq)
#    preamble:
#      - "#SBATCH --gres=gpu:1"
#      - "#SBATCH --partition=unkillable"
#    paralleljobs:
## baseline
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=stsb train.epochs=10 train.batch_size=32 fnmodel.name=none train.lr=2e-5
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=stsb train.epochs=10 train.batch_size=32 fnmodel.name=none train.lr=2e-4
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=stsb train.epochs=10 train.batch_size=32 fnmodel.name=none train.lr=2e-3
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=stsb train.epochs=10 train.batch_size=32 fnmodel.name=ia3 fnmodel.params.rank=2 train.lr=2e-5
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=stsb train.epochs=10 train.batch_size=32 fnmodel.name=ia3 fnmodel.params.rank=2 train.lr=2e-4
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=stsb train.epochs=10 train.batch_size=32 fnmodel.name=ia3 fnmodel.params.rank=2 train.lr=2e-3
## rank2
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=stsb train.epochs=10  train.batch_size=16 fnmodel.name=lora fnmodel.params.rank=2 train.lr=2e-5
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=stsb train.epochs=10  train.batch_size=16 fnmodel.name=lora fnmodel.params.rank=2 train.lr=2e-4
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=stsb train.epochs=10  train.batch_size=16 fnmodel.name=lora fnmodel.params.rank=2 train.lr=2e-3
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=stsb train.epochs=10  train.batch_size=16 fnmodel.name=rosa fnmodel.params.rank=2 train.lr=2e-5
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=stsb train.epochs=10  train.batch_size=16 fnmodel.name=rosa fnmodel.params.rank=2 train.lr=2e-4
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=stsb train.epochs=10  train.batch_size=16 fnmodel.name=rosa fnmodel.params.rank=2 train.lr=2e-3
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=stsb train.epochs=10  train.batch_size=16 fnmodel.name=rosa fnmodel.params.rank=2 train.lr=2e-5 fnmodel.factorize_freq=2
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=stsb train.epochs=10  train.batch_size=16 fnmodel.name=rosa fnmodel.params.rank=2 train.lr=2e-4 fnmodel.factorize_freq=2
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=stsb train.epochs=10  train.batch_size=16 fnmodel.name=rosa fnmodel.params.rank=2 train.lr=2e-3 fnmodel.factorize_freq=2
## rank8
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=stsb train.epochs=10  train.batch_size=16 fnmodel.name=lora fnmodel.params.rank=8 train.lr=2e-5
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=stsb train.epochs=10  train.batch_size=16 fnmodel.name=lora fnmodel.params.rank=8 train.lr=2e-4
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=stsb train.epochs=10  train.batch_size=16 fnmodel.name=lora fnmodel.params.rank=8 train.lr=2e-3
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=stsb train.epochs=10  train.batch_size=16 fnmodel.name=rosa fnmodel.params.rank=8 train.lr=2e-5
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=stsb train.epochs=10  train.batch_size=16 fnmodel.name=rosa fnmodel.params.rank=8 train.lr=2e-4
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=stsb train.epochs=10  train.batch_size=16 fnmodel.name=rosa fnmodel.params.rank=8 train.lr=2e-3
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=stsb train.epochs=10  train.batch_size=16 fnmodel.name=rosa fnmodel.params.rank=8 train.lr=2e-5 fnmodel.factorize_freq=2
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=stsb train.epochs=10  train.batch_size=16 fnmodel.name=rosa fnmodel.params.rank=8 train.lr=2e-4 fnmodel.factorize_freq=2
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=stsb train.epochs=10  train.batch_size=16 fnmodel.name=rosa fnmodel.params.rank=8 train.lr=2e-3 fnmodel.factorize_freq=2

#  - name: GLUE | SST2 | Roberta-Base (peft-method, rank, lr, factorization_freq)
#    preamble:
#      - "#SBATCH --gres=gpu:1"
#      - "#SBATCH --partition=unkillable"
#    paralleljobs:
## baseline
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=sst2 train.epochs=10 train.batch_size=32 fnmodel.name=none train.lr=2e-5
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=sst2 train.epochs=10 train.batch_size=32 fnmodel.name=none train.lr=2e-4
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=sst2 train.epochs=10 train.batch_size=32 fnmodel.name=none train.lr=2e-3
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=sst2 train.epochs=10 train.batch_size=32 fnmodel.name=ia3 fnmodel.params.rank=2 train.lr=2e-5
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=sst2 train.epochs=10 train.batch_size=32 fnmodel.name=ia3 fnmodel.params.rank=2 train.lr=2e-4
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=sst2 train.epochs=10 train.batch_size=32 fnmodel.name=ia3 fnmodel.params.rank=2 train.lr=2e-3
## rank2
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=sst2 train.epochs=10  train.batch_size=16 fnmodel.name=lora fnmodel.params.rank=2 train.lr=2e-5
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=sst2 train.epochs=10  train.batch_size=16 fnmodel.name=lora fnmodel.params.rank=2 train.lr=2e-4
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=sst2 train.epochs=10  train.batch_size=16 fnmodel.name=lora fnmodel.params.rank=2 train.lr=2e-3
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=sst2 train.epochs=10  train.batch_size=16 fnmodel.name=rosa fnmodel.params.rank=2 train.lr=2e-5
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=sst2 train.epochs=10  train.batch_size=16 fnmodel.name=rosa fnmodel.params.rank=2 train.lr=2e-4
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=sst2 train.epochs=10  train.batch_size=16 fnmodel.name=rosa fnmodel.params.rank=2 train.lr=2e-3
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=sst2 train.epochs=10  train.batch_size=16 fnmodel.name=rosa fnmodel.params.rank=2 train.lr=2e-5 fnmodel.factorize_freq=2
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=sst2 train.epochs=10  train.batch_size=16 fnmodel.name=rosa fnmodel.params.rank=2 train.lr=2e-4 fnmodel.factorize_freq=2
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=sst2 train.epochs=10  train.batch_size=16 fnmodel.name=rosa fnmodel.params.rank=2 train.lr=2e-3 fnmodel.factorize_freq=2
# rank8
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=sst2 train.epochs=10  train.batch_size=16 fnmodel.name=lora fnmodel.params.rank=8 train.lr=2e-5
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=sst2 train.epochs=10  train.batch_size=16 fnmodel.name=lora fnmodel.params.rank=8 train.lr=2e-4
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=sst2 train.epochs=10  train.batch_size=16 fnmodel.name=lora fnmodel.params.rank=8 train.lr=2e-3
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=sst2 train.epochs=10  train.batch_size=16 fnmodel.name=rosa fnmodel.params.rank=8 train.lr=2e-5
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=sst2 train.epochs=10  train.batch_size=16 fnmodel.name=rosa fnmodel.params.rank=8 train.lr=2e-4
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=sst2 train.epochs=10  train.batch_size=16 fnmodel.name=rosa fnmodel.params.rank=8 train.lr=2e-3
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=sst2 train.epochs=10  train.batch_size=16 fnmodel.name=rosa fnmodel.params.rank=8 train.lr=2e-5 fnmodel.factorize_freq=2
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=sst2 train.epochs=10  train.batch_size=16 fnmodel.name=rosa fnmodel.params.rank=8 train.lr=2e-4 fnmodel.factorize_freq=2
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=sst2 train.epochs=10  train.batch_size=16 fnmodel.name=rosa fnmodel.params.rank=8 train.lr=2e-3 fnmodel.factorize_freq=2


#  - name: GLUE | wic | Roberta-Base (peft-method, rank, lr, factorization_freq)
#    preamble:
#      - "#SBATCH --gres=gpu:1"
#      - "#SBATCH --partition=unkillable"
#    paralleljobs:
## baseline
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=wic train.epochs=10 train.batch_size=32 fnmodel.name=none train.lr=2e-5
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=wic train.epochs=10 train.batch_size=32 fnmodel.name=none train.lr=2e-4
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=wic train.epochs=10 train.batch_size=32 fnmodel.name=none train.lr=2e-3
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=wic train.epochs=10 train.batch_size=32 fnmodel.name=ia3 fnmodel.params.bias_requires_grad=False fnmodel.params.rank=2 train.lr=2e-5
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=wic train.epochs=10 train.batch_size=32 fnmodel.name=ia3 fnmodel.params.bias_requires_grad=False fnmodel.params.rank=2 train.lr=2e-4
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=wic train.epochs=10 train.batch_size=32 fnmodel.name=ia3 fnmodel.params.bias_requires_grad=False fnmodel.params.rank=2 train.lr=2e-3
## rank2
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=wic train.epochs=10  train.batch_size=16 fnmodel.name=lora fnmodel.params.bias_requires_grad=False fnmodel.params.rank=2 train.lr=2e-5
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=wic train.epochs=10  train.batch_size=16 fnmodel.name=lora fnmodel.params.bias_requires_grad=False fnmodel.params.rank=2 train.lr=2e-4
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=wic train.epochs=10  train.batch_size=16 fnmodel.name=lora fnmodel.params.bias_requires_grad=False fnmodel.params.rank=2 train.lr=2e-3
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=wic train.epochs=10  train.batch_size=16 fnmodel.name=rosa fnmodel.params.bias_requires_grad=False fnmodel.params.rank=2 train.lr=2e-5
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=wic train.epochs=10  train.batch_size=16 fnmodel.name=rosa fnmodel.params.bias_requires_grad=False fnmodel.params.rank=2 train.lr=2e-4
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=wic train.epochs=10  train.batch_size=16 fnmodel.name=rosa fnmodel.params.bias_requires_grad=False fnmodel.params.rank=2 train.lr=2e-3
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=wic train.epochs=10  train.batch_size=16 fnmodel.name=rosa fnmodel.params.bias_requires_grad=False fnmodel.params.rank=2 train.lr=2e-5 fnmodel.factorize_freq=2
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=wic train.epochs=10  train.batch_size=16 fnmodel.name=rosa fnmodel.params.bias_requires_grad=False fnmodel.params.rank=2 train.lr=2e-4 fnmodel.factorize_freq=2
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=wic train.epochs=10  train.batch_size=16 fnmodel.name=rosa fnmodel.params.bias_requires_grad=False fnmodel.params.rank=2 train.lr=2e-3 fnmodel.factorize_freq=2
## rank8
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=wic train.epochs=10  train.batch_size=16 fnmodel.name=lora fnmodel.params.bias_requires_grad=False fnmodel.params.rank=8 train.lr=2e-5
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=wic train.epochs=10  train.batch_size=16 fnmodel.name=lora fnmodel.params.bias_requires_grad=False fnmodel.params.rank=8 train.lr=2e-4
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=wic train.epochs=10  train.batch_size=16 fnmodel.name=lora fnmodel.params.bias_requires_grad=False fnmodel.params.rank=8 train.lr=2e-3
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=wic train.epochs=10  train.batch_size=16 fnmodel.name=rosa fnmodel.params.bias_requires_grad=False fnmodel.params.rank=8 train.lr=2e-5
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=wic train.epochs=10  train.batch_size=16 fnmodel.name=rosa fnmodel.params.bias_requires_grad=False fnmodel.params.rank=8 train.lr=2e-4
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=wic train.epochs=10  train.batch_size=16 fnmodel.name=rosa fnmodel.params.bias_requires_grad=False fnmodel.params.rank=8 train.lr=2e-3
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=wic train.epochs=10  train.batch_size=16 fnmodel.name=rosa fnmodel.params.bias_requires_grad=False fnmodel.params.rank=8 train.lr=2e-5 fnmodel.factorize_freq=2
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=wic train.epochs=10  train.batch_size=16 fnmodel.name=rosa fnmodel.params.bias_requires_grad=False fnmodel.params.rank=8 train.lr=2e-4 fnmodel.factorize_freq=2
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=wic train.epochs=10  train.batch_size=16 fnmodel.name=rosa fnmodel.params.bias_requires_grad=False fnmodel.params.rank=8 train.lr=2e-3 fnmodel.factorize_freq=2
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=wic train.epochs=10  train.batch_size=16 fnmodel.name=rosa fnmodel.params.bias_requires_grad=False fnmodel.params.rank=8 train.lr=2e-5 fnmodel.factorize_freq=3
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=wic train.epochs=10  train.batch_size=16 fnmodel.name=rosa fnmodel.params.bias_requires_grad=False fnmodel.params.rank=8 train.lr=2e-4 fnmodel.factorize_freq=3
#      - python train_mlm.py dataset.cache=$SLURM_TMPDIR/huggingface +profile=marawan seed=42 +task=wic train.epochs=10  train.batch_size=16 fnmodel.name=rosa fnmodel.params.bias_requires_grad=False fnmodel.params.rank=8 train.lr=2e-3 fnmodel.factorize_freq=3